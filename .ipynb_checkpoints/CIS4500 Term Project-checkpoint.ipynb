{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document length in sentences min: 2 max: 113 average: 33.36\n",
      "document length in tokens min: 18 max: 2753 average: 762.5195\n",
      "document sentence lengths in collection 23.37124054883429\n"
     ]
    }
   ],
   "source": [
    "#read from files and create a dataframe of data\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from nltk import word_tokenize\n",
    "\n",
    "neg_list = glob.glob(\"./data/neg/*.txt\")\n",
    "pos_list = glob.glob(\"./data/pos/*.txt\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "doc_list = []\n",
    "filename_list = []\n",
    "\n",
    "#reading the data\n",
    "for file in neg_list:\n",
    "    file1 = open(file,\"r\").read()\n",
    "    doc_list.append([file1, 0])\n",
    "    filename_list.append(os.path.basename(file))\n",
    "\n",
    "for file in pos_list:\n",
    "    file1 = open(file,\"r\").read()\n",
    "    doc_list.append([file1, 1])\n",
    "    filename_list.append(os.path.basename(file))\n",
    "    \n",
    "total_avg_sentences = 0\n",
    "total_sentences = 0\n",
    "total_tokens = 0\n",
    "min_sentence = 10000\n",
    "max_sentence = 0\n",
    "min_tokens = 10000\n",
    "max_tokens = 0\n",
    "\n",
    "for x in range(len(filename_list)):\n",
    "    sentences = doc_list[x][0].split('\\n')\n",
    "    tokens = word_tokenize(doc_list[x][0])\n",
    "    \n",
    "    total_avg_sentences += len(tokens) / len(sentences)\n",
    "    \n",
    "    total_sentences += len(sentences) \n",
    "    total_tokens += len(tokens)\n",
    "    \n",
    "    if (len(sentences) > max_sentence):\n",
    "        max_sentence = len(sentences)\n",
    "    if (len(sentences) < min_sentence):\n",
    "        min_sentence = len(sentences)\n",
    "        \n",
    "    if (len(tokens) > max_tokens):\n",
    "        max_tokens = len(tokens)\n",
    "    if (len(tokens) < min_tokens):\n",
    "        min_tokens = len(tokens)\n",
    "\n",
    "print(\"document length in sentences\", \"min:\", min_sentence, \"max:\", max_sentence, \"average:\", total_sentences/len(filename_list))\n",
    "print(\"document length in tokens\", \"min:\", min_tokens, \"max:\", max_tokens, \"average:\", total_tokens/len(filename_list))\n",
    "print(\"document sentence lengths in collection\", total_avg_sentences/len(filename_list))\n",
    "    \n",
    "data = pd.DataFrame(doc_list, columns = ['text' , 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set cleaner which removes punctuation, and converts everything to lower case\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def clean_dataset(text):\n",
    "    lower_case = text.lower()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", lower_case)\n",
    "    tokens = tokenizer.tokenize(letters_only)\n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1597.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "def post_process(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['text'] = data['text'].progress_map(clean_dataset)  \n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = post_process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our model into test_set and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 1234\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(data.text, data.sentiment, test_size=0.15, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selecting using Count Vectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.set_params(stop_words=stop_words, max_features=2000)\n",
    "\n",
    "#building our pipeline for count vectorizer and logistic regression\n",
    "lg_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "#feature selecting using Tf-idf Vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.set_params(stop_words=stop_words, max_features=2000)\n",
    "\n",
    "#building our pipeline for tfidf and logistic regression\n",
    "lg_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', LogisticRegression(solver='liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building our pipeline for count vectorizer and Ridge Classifier\n",
    "rc_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', RidgeClassifier())])\n",
    "#building our pipeline for tfidf and Ridge Classifier\n",
    "rc_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', RidgeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building our pipeline for count vectorizer and Nearest Centroid\n",
    "nc_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', NearestCentroid())])\n",
    "#building our pipeline for tfidf and Nearest Centroid\n",
    "nc_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', NearestCentroid())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgcv = cross_val_score(lg_cv_pipeline, x_train, y_train, cv=5)\n",
    "lg_tfidf= cross_val_score(lg_tfidf_pipeline, x_train, y_train, cv=5)\n",
    "rccv = cross_val_score(rc_cv_pipeline, x_train, y_train, cv=5)\n",
    "rctfidf = cross_val_score(rc_tfidf_pipeline, x_train, y_train, cv=5)\n",
    "nccv = cross_val_score(nc_cv_pipeline, x_train, y_train, cv=5)\n",
    "nctfidf = cross_val_score(nc_tfidf_pipeline, x_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164705882352941 0.8288235294117646 0.7323529411764707 0.8299999999999998 0.6717647058823529 0.8011764705882353\n"
     ]
    }
   ],
   "source": [
    "print(lgcv.mean(), lg_tfidf.mean(), rccv.mean(), rctfidf.mean(), nccv.mean(), nctfidf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.741764705882353 0.7858823529411765 0.7405882352941177 0.7688235294117647 0.6535294117647059 0.7623529411764706\n",
      "1000 0.7911764705882354 0.8170588235294117 0.6941176470588235 0.8152941176470587 0.6623529411764706 0.7817647058823529\n",
      "1500 0.7988235294117647 0.8400000000000001 0.6841176470588236 0.8305882352941175 0.6641176470588236 0.7970588235294118\n",
      "2000 0.8135294117647058 0.8470588235294118 0.7305882352941176 0.8358823529411765 0.666470588235294 0.8070588235294117\n",
      "2500 0.8164705882352941 0.8441176470588235 0.7476470588235294 0.8382352941176471 0.6688235294117646 0.8047058823529412\n",
      "3000 0.8123529411764705 0.8405882352941175 0.7641176470588235 0.8358823529411763 0.6729411764705882 0.8017647058823529\n"
     ]
    }
   ],
   "source": [
    "for max_features in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "    cv.set_params(stop_words=stop_words, max_features=max_features)\n",
    "    tfidf.set_params(stop_words=stop_words, max_features=max_features)\n",
    "    lg_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "    lg_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "    rc_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', RidgeClassifier())])\n",
    "    rc_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', RidgeClassifier())])\n",
    "    lgcv = cross_val_score(lg_cv_pipeline, x_train, y_train, cv=10)\n",
    "    lg_tfidf= cross_val_score(lg_tfidf_pipeline, x_train, y_train, cv=10)\n",
    "    rccv = cross_val_score(rc_cv_pipeline, x_train, y_train, cv=10)\n",
    "    rctfidf = cross_val_score(rc_tfidf_pipeline, x_train, y_train, cv=10)\n",
    "    nccv = cross_val_score(nc_cv_pipeline, x_train, y_train, cv=10)\n",
    "    nctfidf = cross_val_score(nc_tfidf_pipeline, x_train, y_train, cv=10)\n",
    "    print(max_features, lgcv.mean(), lg_tfidf.mean(), rccv.mean(), rctfidf.mean(), nccv.mean(), nctfidf.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**500** &ensp;0.741764705882353 0.7858823529411765 0.74 0.7688235294117647 0.6535294117647059 0.7623529411764706<br>\n",
    "**1000** 0.7911764705882354 0.8170588235294117 0.6941176470588235 0.8158823529411764 0.6623529411764706 0.7817647058823529<br>\n",
    "**1500** 0.7988235294117647 0.8400000000000001 0.6847058823529413 0.8305882352941175 0.6641176470588236 0.7970588235294118<br>\n",
    "**2000** 0.8135294117647058 **0.8470588235294118** 0.7305882352941176 0.8358823529411765 0.666470588235294 0.8070588235294117<br>\n",
    "**2500** 0.8164705882352941 0.8441176470588235 0.7476470588235294 **0.8388235294117647** 0.6688235294117646 0.8047058823529412<br>\n",
    "**3000** 0.8123529411764705 0.8405882352941175 0.7652941176470588 0.8358823529411763 0.6729411764705882 0.8017647058823529<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 300\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Based on the results it becomes very evident that **TFIDF feature selection method** is more accurate than using a count vectorization feature selection method.\n",
    "\n",
    "Increasing the amount of maximum features selected gradually improves the performance of the models up until the 2000 features mark. Beyond that we see very marginal improvements in performance, and in some cases a decrease in performance. Furthermore, increasing the amount of features we look at also increases the time and complexity of the model overall. Therefore, our most favorable selection would be somewhere between **2000 and 2500 maximum features**. \n",
    "\n",
    "Between the 3 models that were used it becomes evident that **Logistic Regression and Ridge Classifiers** both provide very similar performance slightly beating out Nearest Centroid \n",
    "\n",
    "Judging from our results above we can see that the logistic regression using the TFIDF feature selection with maximum features of 2000 method yields the best result.\n",
    "\n",
    "We can also observe the Ridge classifier using the TFIDF feature selection with maximum features of 2500 also yields some good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score is 0.8299981110901232\n",
      "The precision score is 0.8300146673185476\n",
      "The recal score is 0.8300000000000001\n",
      "[[124  26]\n",
      " [ 25 125]]\n"
     ]
    }
   ],
   "source": [
    "#testing our first \"Good\" model\n",
    "\n",
    "tfidf.set_params(stop_words=stop_words, max_features=2000)\n",
    "lg_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "model = lg_tfidf_pipeline.fit(x_train, y_train)\n",
    "result = model.predict(x_validate)\n",
    "print('The f1 score is', f1_score(y_validate, result, average=\"macro\"))\n",
    "print('The precision score is', precision_score(y_validate, result, average=\"macro\"))\n",
    "print('The recal score is', recall_score(y_validate, result, average=\"macro\")) \n",
    "print(confusion_matrix(y_validate, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score is 0.8399715504978664\n",
      "The precision score is 0.840241949830991\n",
      "The recal score is 0.8400000000000001\n",
      "[[128  22]\n",
      " [ 26 124]]\n"
     ]
    }
   ],
   "source": [
    "#testing our second \"Good\" model\n",
    "\n",
    "tfidf.set_params(stop_words=stop_words, max_features=2500)\n",
    "rc_tfidf_pipeline = Pipeline([('vectorizer', tfidf), ('classifier', RidgeClassifier())])\n",
    "model = rc_tfidf_pipeline.fit(x_train, y_train)\n",
    "result = model.predict(x_validate)\n",
    "print('The f1 score is', f1_score(y_validate, result, average=\"macro\"))\n",
    "print('The precision score is', precision_score(y_validate, result, average=\"macro\"))\n",
    "print('The recal score is', recall_score(y_validate, result, average=\"macro\")) \n",
    "print(confusion_matrix(y_validate, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score is 0.6550324675324675\n",
      "The precision score is 0.669779286926995\n",
      "The recal score is 0.66\n",
      "[[117  33]\n",
      " [ 69  81]]\n"
     ]
    }
   ],
   "source": [
    "#testing one of our worst model\n",
    "\n",
    "cv.set_params(stop_words=stop_words, max_features=500)\n",
    "nc_cv_pipeline = Pipeline([('vectorizer', cv), ('classifier', NearestCentroid())])\n",
    "model = nc_cv_pipeline.fit(x_train, y_train)\n",
    "result = model.predict(x_validate)\n",
    "print('The f1 score is', f1_score(y_validate, result, average=\"macro\"))\n",
    "print('The precision score is', precision_score(y_validate, result, average=\"macro\"))\n",
    "print('The recal score is', recall_score(y_validate, result, average=\"macro\")) \n",
    "print(confusion_matrix(y_validate, result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
