{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "neg_list = glob.glob(\"./data/neg/*.txt\")\n",
    "pos_list = glob.glob(\"./data/pos/*.txt\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "doc_list = []\n",
    "\n",
    "#reading the data\n",
    "for file in neg_list:\n",
    "    file1 = open(file,\"r\").read()\n",
    "    doc_list.append([file1, 0])\n",
    "\n",
    "for file in pos_list:\n",
    "    file1 = open(file,\"r\").read()\n",
    "    doc_list.append([file1, 1])\n",
    "    \n",
    "data = pd.DataFrame(doc_list, columns = ['text' , 'sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_dataset(text):\n",
    "    lower_case = text.lower()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", lower_case)\n",
    "    tokens = tokenizer.tokenize(letters_only)\n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "def post_process(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['text'] = data['text'].progress_map(clean_dataset)  \n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = post_process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 1234\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.text, data.sentiment, test_size=.15, random_state=SEED)\n",
    "\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "def acc_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy, train_test_time\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n",
    "         \"Bernoulli NB\", \"Ridge Classifier\", \"AdaBoost\", \"Perceptron\",\"Passive-Aggresive\", \"Nearest Centroid\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    RidgeClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    NearestCentroid()\n",
    "    ]\n",
    "zipped_clf = zip(names,classifiers)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "def classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        print(\"Validation result for {}\".format(n))\n",
    "        print(c)\n",
    "        clf_acc,tt_time = acc_summary(checker_pipeline, x_train, y_train, x_test, y_test)\n",
    "        result.append((n,clf_acc,tt_time))\n",
    "    return result\n",
    "\n",
    "trigram_result = classifier_comparator(n_features=100000,ngram_range=(1,3))\n",
    "\n",
    "print(trigram_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import string\n",
    "import re\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "pos_word_freq = {}\n",
    "neg_word_freq = {}\n",
    "for document in doc_list:\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = word_tokenize(document[0]) \n",
    "    tokens = [x for x in tokens if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "    for token in tokens:\n",
    "        \n",
    "        if (document[1] is 0):\n",
    "            if token not in neg_word_freq.keys():\n",
    "                neg_word_freq[token] = 1\n",
    "            else:\n",
    "                neg_word_freq[token] += 1\n",
    "        else:\n",
    "            if token not in pos_word_freq.keys():\n",
    "                pos_word_freq[token] = 1\n",
    "            else:\n",
    "                pos_word_freq[token] += 1\n",
    "            \n",
    "print(len(neg_word_freq.keys()))\n",
    "print(neg_word_freq)\n",
    "\n",
    "print(len(pos_word_freq.keys()))\n",
    "print(pos_word_freq)\n",
    "\n",
    "#most_freq = heapq.nlargest(200, word_freq, key=word_freq.get)\n",
    "#print(most_freq)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
